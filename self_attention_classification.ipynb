{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self_attention_classification",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_PjtofYd4yB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2fntS-PbsvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo8vLpdmdoBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DbJa-7rdoI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeEoRk1_4ZZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TQQxYSY5iWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_lines_neu = []\n",
        "categories_neu = {}\n",
        "categories_neg = {}\n",
        "categories_pos = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQBzNQpR5kNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('TA_NEU.txt','r') as f:\n",
        "  lines_neu = f.readlines()\n",
        "  lines_neu = lines_neu[65:]\n",
        "  for line in lines_neu:\n",
        "      new_lines = line.split()\n",
        "      categories_neu[new_lines[1]] = [0,0,1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2QxwC-u550D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('TA_NEG.txt','r') as f:\n",
        "  lines_neg = f.readlines()\n",
        "  lines_neg = lines_neg[65:]\n",
        "  for line in lines_neg:\n",
        "      new_lines = line.split()\n",
        "      categories_neg[new_lines[1]] = [0,1,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyUXjd165_3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('TA_POS.txt','r') as f:\n",
        "  lines_pos = f.readlines()\n",
        "  lines_pos = lines_pos[65:]\n",
        "  for line in lines_pos:\n",
        "      new_lines = line.split()\n",
        "      categories_pos[new_lines[1]] = [1,0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whe6ocGu6EAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merge = [categories_neg,categories_neu,categories_pos]\n",
        "new_categories = {}\n",
        "for x in merge:\n",
        "  new_categories.update(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v15v3Pqk6Sr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys = list(new_categories.keys())\n",
        "random.shuffle(keys)\n",
        "new_categories1 = [(key, new_categories[key]) for key in keys]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92i2xQp_6VDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_categories1 = dict(new_categories1)\n",
        "len(new_categories1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUMxTaDE1C5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AZ6oVxED8Ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys = list(new_categories1.keys())\n",
        "for i,k in new_categories1.items():\n",
        "  categories.append(k)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXdL_Shf0bB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVR2ahmxD_j0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_letters = set('அஇஉஎஒஆஈஊஏஓஐஔங்ங்ஞ்ண்ந்ம்ன்க்ச்ட்த்ப்ற்ய்ர்ல்வ்ழ்ள்ஃ')\n",
        "all_letters = list(all_letters)\n",
        "len(all_letters)\n",
        "n_letters = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dCAlOuE-Jxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,bacth_size,output_size,hidden_size,vocab_size,embedding_length):\n",
        "    super(SelfAttention,self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_length = embedding_length\n",
        "    \n",
        "    self.word_embeddings = nn.Embedding(vocab_size,embedding_length)\n",
        "\n",
        "    self.bilstm = nn.LSTM(embedding_length, hidden_size, bidirectional=False)\n",
        "    self.W_s1 = nn.Linear(hidden_size, 350)\n",
        "    self.W_s2 = nn.Linear(350, 30)\n",
        "    self.fc_layer = nn.Linear(30*hidden_size, 400)\n",
        "    self.label = nn.Linear(400, output_size)\n",
        "  def attention_net(self,lstm_output):\n",
        "    attn_weight_matrix = self.W_s2(torch.tanh(self.W_s1(lstm_output)))\n",
        "    attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "    attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "    return attn_weight_matrix\n",
        "  def forward(self, input_sentences, batch_size=1):\n",
        "    input_sentences = input_sentences.long()\n",
        "    inputs = self.word_embeddings(input_sentences)\n",
        "    inputs = inputs.squeeze(2)\n",
        "    h_0 = torch.zeros(1, 32, self.hidden_size)\n",
        "    c_0 = torch.zeros(1, 32, self.hidden_size)\n",
        "    output, (h_n, c_n) = self.bilstm(inputs, (h_0, c_0))\n",
        "    output = output.permute(1, 0, 2)\n",
        "    attn_weight_matrix = self.attention_net(output)\n",
        "    hidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
        "    fc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
        "    logits = self.label(fc_out)\n",
        "    return logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt6N1G2wd0a_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "def letterToIndex(letter):\n",
        "  index = -1\n",
        "  try:\n",
        "    for i,k in enumerate(all_letters):\n",
        "      if letter == k:\n",
        "        return i\n",
        "  except:\n",
        "    print('please enter a valid character')\n",
        "\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        if letterToIndex(letter) != -1:\n",
        "          tensor[li][0][letterToIndex(letter)] = 1\n",
        "        \n",
        "    return tensor\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brKQyc2tva_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = n_letters\n",
        "embedding = n_letters\n",
        "output_size = 3\n",
        "batch_size = 1\n",
        "hidden_size = 256\n",
        "\n",
        "\n",
        "attention = SelfAttention(batch_size, output_size, hidden_size, vocab_size,embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kno47cLGwLAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    l = [0,0,0]\n",
        "    l[category_i] = l[category_i]+1\n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDf3937cnBQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(params = attention.parameters(),lr = 0.5)\n",
        " \n",
        "def train(category_tensors, line_tensors):\n",
        "  final_loss = 0\n",
        "  correct_count = 0\n",
        "  for i in range(0,100):\n",
        "    line_tensor = line_tensors[i]\n",
        "    category = category_tensors[i] \n",
        "    category_tensor = torch.tensor(category, dtype=torch.float)\n",
        "   \n",
        "    \n",
        "    for j in range(line_tensor.size()[0]):\n",
        "        output = attention(line_tensor[j].unsqueeze(2))\n",
        "    idx = torch.topk(output, k=1, dim=0)[1]     \n",
        "    guess_i = categoryFromOutput(idx)\n",
        "    \n",
        "    if guess_i == category:\n",
        "      correct =  '✓'\n",
        "      correct_count = correct_count + 1\n",
        "    else:\n",
        "      correct =  '✗ (%s)' % category \n",
        "  \n",
        "    category_tensor = category_tensor.unsqueeze(0)\n",
        "    for i in range(5):\n",
        "      category_tensor = torch.cat((category_tensor,category_tensor),0)\n",
        "\n",
        "    loss = criterion(output, category_tensor)\n",
        "  \n",
        "    final_loss = final_loss+loss\n",
        "  final_loss = final_loss/100.0\n",
        "\n",
        "  final_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return final_loss,correct_count   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lbHxDkH0Aaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_iters = 4000\n",
        "cat_tensors = []\n",
        "line_tensors = []\n",
        "all_losses = []\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  batch_num = 0\n",
        "  correct_sum = 0\n",
        "  final_loss = 0\n",
        "  attention.zero_grad()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  for iter in range(0,n_iters+1):\n",
        "    \n",
        "    while True:\n",
        "      \n",
        "      category = new_categories1[keys[iter]]\n",
        "      cat_tensors.append(category)\n",
        "      line_tensor = lineToTensor(keys[iter])\n",
        "      line_tensors.append(line_tensor)\n",
        "      if iter % 100 != 0 or iter == 0:\n",
        "        break\n",
        "      else:\n",
        "        batch_num = batch_num + 1\n",
        "        final_loss,correct = train(cat_tensors,line_tensors)\n",
        "        correct_sum = correct_sum + correct\n",
        "        cat_tensors.clear()\n",
        "        line_tensors.clear()\n",
        "        if iter != 0:\n",
        "          print('%d %s %.4f  %s %s' % (batch_num, iter , final_loss, correct_sum,epoch))\n",
        "          break\n",
        "  print( 'accuracy is %.6f%%' % (correct_sum/4000 * 100))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFXCPFwDvKjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate2(line_tensor):\n",
        "    \"\"\"This function is used to evaluate the model\"\"\"\n",
        "   \n",
        "    for j in range(line_tensor.size()[0]):\n",
        "      output = attention(line_tensor[j].unsqueeze(2))   \n",
        "       \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9-9cMszKzZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_corr2 = 0\n",
        "n_iters = 4000\n",
        "#Here we evaluate the testing data\n",
        "for i in range(n_iters+1,6202):\n",
        "  \n",
        "  category = new_categories1[keys[i]]\n",
        "  category_tensor = torch.tensor(category, dtype=torch.float)\n",
        "  line_tensor = lineToTensor(keys[i])\n",
        "  output = evaluate2(line_tensor)\n",
        "  #idx = torch.topk(output, k=1, dim=0)[1] \n",
        "  guess_new = categoryFromOutput(output)  \n",
        "  if guess_new == category:\n",
        "    correct =  '✓'\n",
        "    num_corr2 = num_corr2+ 1\n",
        "  else:\n",
        "    correct =  '✗ (%s)' % category \n",
        "  print('%d %d%% %s' % (i, i / n_iters * 100,correct))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUlk42eHK9ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we are the printing the accuracy of the testing data \n",
        "print(num_corr2/2200 * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coDWZodQMSFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5toUJgnPV82M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoywsxzxV87S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdJhOZuiV9BW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tue2SAP_V9EI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE0BMNjpV8_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O22-OeCcV843",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}